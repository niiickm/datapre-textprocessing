{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "News sources: (Date collected: 04-05-2021)\n",
    "    - Inquirer\n",
    "    - Manila Bulletin\n",
    "    - The Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all news articles into one json\n",
    "\n",
    "The files were taken from the previous homework, scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News articles from Inquirer:  53\n",
      "News articles from Manila Bulletin:  27\n",
      "News articles from The Guardian:  49\n",
      "Total number of news articles:  129\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "\n",
    "read = []\n",
    "\n",
    "with open('inquirer_news.json') as f:\n",
    "    read = json.load(f)\n",
    "    print(\"News articles from Inquirer: \", len(read))\n",
    "\n",
    "articles.extend(read)\n",
    "\n",
    "with open('mb_news.json') as f:\n",
    "    read = json.load(f)\n",
    "    print(\"News articles from Manila Bulletin: \", len(read))\n",
    "\n",
    "\n",
    "articles.extend(read)\n",
    "\n",
    "with open('guardian_news.json') as f:\n",
    "    read = json.load(f)\n",
    "    print(\"News articles from The Guardian: \", len(read))\n",
    "\n",
    "articles.extend(read)\n",
    "print(\"Total number of news articles: \", len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0\n",
      "\n",
      "Source:  https://newsinfo.inquirer.net/1415144/healthcare-utilization-should-drop-to-60-before-easing-of-ncr-plus-restrictions-doh\n",
      "\n",
      "Title:  To ease ‘NCR Plus’ curbs, healthcare demand must drop to 60% – DOH\n",
      "\n",
      "Author:  Christia Marie Ramos\n",
      "\n",
      "Date:  7:49 PM April 05, 2021\n",
      "\n",
      "Article body: \n",
      " \n",
      "MANILA, Philippines — The current restrictions enforced on the “NCR Plus” area can be eased once healthcare demand has been lowered to at least 60 percent, the Department of Health (DOH) said Monday.\n",
      "“For healthcare utilization, we need to see that the utilization will be down to at least 60% before we can say that we are at that safe level,” DOH Undersecretary Ma. Rosario Vergeire said in a Palace briefing.\n",
      "“The health system should be able to manage and should be able to breathe and should have this decongestion before we can say that we can easily lift the restrictions for this community quarantine,” she added.\n",
      "According to a DOH official, the National Capital Region’s total healthcare demand is reportedly between 78 and 80 percent.\n",
      "“For ICU (intensive care unit) beds…almost 100% in most cities. So we need to bring that down so that we will be able to say that our healthcare system can manage,” she said.\n",
      "“What would be most important really would be for us to look at the healthcare utilization,” she added.\n",
      "Initially, the government imposed the enhanced community quarantine (ECQ), the government’s most stringent quarantine classification, on the “NCR Plus” bubble that encompassed Metro Manila, Bulacan, Cavite, Laguna, and Rizal. The ECQ was scheduled to expire on April 4, but was extended for an additional week.\n",
      "READ:\n",
      "Malacañang, on the other hand, has ruled out extending the ECQ in the affected region for a third week, preferring to impose a more lenient modified ECQ for a week after the existing status expires on April 11.\n",
      "READ:\n",
      "The country has seen record-breaking daily increases in COVID-19 infections during the ECQ, with the highest single-day rise of 15,310 infections recorded last April 2.\n",
      "READ:\n",
      "On Monday, the Philippines’ COVID-19 case tally exceeded 800,00 after the DOH reported 8,355 new infections. Of the total, 143,726 are active cases.\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "article_no = 0\n",
    "\n",
    "print('Index: ', article_no)\n",
    "print('\\nSource: ', articles[article_no]['source'])\n",
    "print(\"\\nTitle: \", articles[article_no]['title'])\n",
    "print(\"\\nAuthor: \", articles[article_no]['author'])\n",
    "print(\"\\nDate: \", articles[article_no]['date'])\n",
    "print(\"\\nArticle body: \\n\", articles[article_no]['article_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\niiick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "['manila', 'philippines', 'current', 'restrictions', 'enforce', 'plus', 'area', 'ease', 'healthcare', 'demand', 'lower', 'percent', 'department', 'health', 'say', 'monday', 'healthcare', 'utilization', 'need', 'utilization', 'safe', 'level', 'undersecretary', 'rosario', 'vergeire', 'say', 'palace', 'brief', 'health', 'able', 'manage', 'able', 'breathe', 'decongestion', 'easily', 'lift', 'restrictions', 'community', 'quarantine', 'add', 'accord', 'official', 'national', 'capital', 'region', 'total', 'healthcare', 'demand', 'reportedly', 'percent', 'intensive', 'care', 'unit', 'bed', 'cities', 'need', 'bring', 'able', 'healthcare', 'manage', 'say', 'important', 'look', 'healthcare', 'utilization', 'add', 'initially', 'government', 'impose', 'enhance', 'community', 'quarantine', 'government', 'stringent', 'quarantine', 'classification', 'plus', 'bubble', 'encompass', 'metro', 'manila', 'bulacan', 'cavite', 'laguna', 'rizal', 'schedule', 'expire', 'april', 'extend', 'additional', 'week', 'read', 'malacañang', 'hand', 'rule', 'extend', 'affect', 'region', 'week', 'prefer', 'impose', 'lenient', 'modify', 'week', 'exist', 'status', 'expire', 'april', 'read', 'country', 'see', 'record', 'break', 'daily', 'increase', 'covid', 'infections', 'highest', 'single', 'rise', 'infections', 'record', 'april', 'read', 'monday', 'philippines', 'covid', 'case', 'tally', 'exceed', 'report', 'infections', 'total', 'active', 'case']\n"
     ]
    }
   ],
   "source": [
    "# Stem / Lemmatize\n",
    "nltk.download('wordnet')\n",
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "# Tokenizing\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "article_bodies = []\n",
    "\n",
    "for x in articles:\n",
    "    article_bodies.append(preprocess(x['article_body']))\n",
    "    \n",
    "print(len(article_bodies))\n",
    "print(article_bodies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : able\n",
      "1 : accord\n",
      "2 : active\n",
      "3 : add\n",
      "4 : additional\n",
      "5 : affect\n",
      "6 : april\n",
      "7 : area\n",
      "8 : bed\n",
      "9 : break\n",
      "10 : breathe\n",
      "11 : brief\n",
      "12 : bring\n",
      "13 : bubble\n",
      "14 : bulacan\n",
      "15 : capital\n",
      "16 : care\n",
      "17 : case\n",
      "18 : cavite\n",
      "19 : cities\n"
     ]
    }
   ],
   "source": [
    "# Creating gensim dictionary\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(article_bodies)\n",
    "\n",
    "for x in range(0, 20):\n",
    "    print(x,\":\",dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Map bag of words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in article_bodies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "len(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0\n",
      "\n",
      "Source:  https://newsinfo.inquirer.net/1415144/healthcare-utilization-should-drop-to-60-before-easing-of-ncr-plus-restrictions-doh\n",
      "\n",
      "Title:  To ease ‘NCR Plus’ curbs, healthcare demand must drop to 60% – DOH\n",
      "\n",
      "Author:  Christia Marie Ramos\n",
      "\n",
      "Date:  7:49 PM April 05, 2021\n",
      "\n",
      "Article body TFIDF: \n",
      " [(0, 0.3375325509797615), (1, 0.08892257853724608), (2, 0.1300135428740754), (3, 0.186894252823476), (4, 0.11945954181625008), (5, 0.11945954181625008), (6, 0.10933758242648152), (7, 0.10933758242648152), (8, 0.10976962409453825), (9, 0.21867516485296304), (10, 0.058467801911536645), (11, 0.11945954181625008), (12, 0.07915764336286867), (13, 0.11945954181625008), (14, 0.0946543040590526), (15, 0.1144951946344838), (16, 0.10933758242648152), (17, 0.3583786254487503), (18, 0.11945954181625008), (19, 0.11945954181625008), (20, 0.1736546918994867), (21, 0.11587655195198257), (22, 0.07782709019206399), (23, 0.08097803838827482), (24, 0.17784515707449217), (25, 0.23891908363250017), (26, 0.15137339213546602), (27, 0.2800989464257332), (28, 0.23891908363250017), (29, 0.19639386508894235), (30, 0.07915764336286867), (31, 0.11587655195198257), (32, 0.11587655195198257), (33, 0.19639386508894235), (34, 0.2667677356117383)]\n"
     ]
    }
   ],
   "source": [
    "articles_tfidf = []\n",
    "\n",
    "for x in range(len(articles)):\n",
    "    article_body_bow = []\n",
    "    for y in corpus_tfidf[x]:\n",
    "        article_body_bow.append((dictionary[]))\n",
    "    articles_tfidf.append({\n",
    "        'source': articles[x]['source'],\n",
    "        'date': articles[x]['date'],\n",
    "        'title': articles[x]['title'],\n",
    "        'author': articles[x]['author'],\n",
    "        'article_body': articles[x]['article_body'],\n",
    "        'article_body_bow': article_body_bow\n",
    "    })\n",
    "    \n",
    "# Sanity check:\n",
    "\n",
    "article_no = 0\n",
    "\n",
    "print('Index: ', article_no)\n",
    "print('\\nSource: ', articles_tfidf[article_no]['source'])\n",
    "print(\"\\nTitle: \", articles_tfidf[article_no]['title'])\n",
    "print(\"\\nAuthor: \", articles_tfidf[article_no]['author'])\n",
    "print(\"\\nDate: \", articles_tfidf[article_no]['date'])\n",
    "print(\"\\nArticle body TFIDF: \\n\", articles_tfidf[article_no]['article_body_bow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
